var documenterSearchIndex = {"docs":
[{"location":"query/#Data-insights","page":"data insights","title":"Data insights","text":"","category":"section"},{"location":"query/#Finance","page":"data insights","title":"Finance","text":"","category":"section"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  given employees are working remote, what are savings in operating expenses, if one fourth of locations in California are permanently closed","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  if employees didn't air travel in 2020, did road travel and lodging expenses remained same","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  how much of travel costs can be expense back to employee as incentives","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  loss in cash flow, income , revenue during 2020","category":"page"},{"location":"query/#Supply-chain","page":"data insights","title":"Supply chain","text":"","category":"section"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  during 2020, what UNSPSC category experience highest surge in source orders","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  during 2020, in \"cleaning, supplies\" category, what are the other items sold more than average","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  what are most requested items in \"cleaning, supplies\" category","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  what are most requested items in \"essentials\" category","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  what are most requested items in \"life savings drugs / GUDID implants\" category","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  was there an decrease in implants, in other words, were they any cardiac or other critical operational surgery delay during covid","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  what are items purchase together with devices in category = ventilators","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  was there an increased use items categorized as \"controlled substance\" items","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  what are the items, which were in backlog most frequently and waited longest","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  what are the items, where receipt was delayed significantly","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query   all those items with increased demands, who are the supplier and their locations","category":"page"},{"location":"query/","page":"data insights","title":"data insights","text":"Query  all those items with increased demands and extended delayed receipts, who are the suppliers and their locations","category":"page"},{"location":"define/#Define-Problem-Statement","page":"Define Problem","title":"Define Problem Statement","text":"","category":"section"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"remember this...","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"(Image: ToiletPaper) (Image: ToiletPaper) (Image: ToiletPaper) (Image: ToiletPaper)","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"Just to be clear, This book is really about Data Science, Graph Analysis to understand, how something like toilet paper is sold as gold during Pandemic.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"I am sure, There are thousands of other items like Ventilators which are lot more essential than toilet papers, and certainly most critical for survivals. Toilet paper may be least popular item which are needed in essentials, but is a symbol of cleanliness to start with.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"One can't deny, in recent time, AI Advance Predictive Analytics dominate market conditions. Most of the organizations now a days can predict their capital purchases, sales with great accuracy and plan manufacturing accordingly. Yet, These organizations and Advance technologies failed to predict and manage life essentials supply chain shortage during recent covid pandemic. Not because, We don't have the capabilities, it was due to lack of monetary benefits realized.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"I can guarantee, we have Technologies to predict and manage any supply chain shortages at any time, what we lack is, intent to use these technologies to a good use due to lesser ROI.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"In next few chapters, we will use data science technologies to understand, predict and perhaps prevent global supply chain shortages specially for those items which every person needs to survive. And this is sole objective of this book.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"If you are still not convinced that solving toilet paper, cleaning items or other life saving essentials, supply chain crisis is a real problem then let me share a real story.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"If you are still not convinced that solving toilet paper supply chain crisis is a real problem then let me share a real story.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"January 1, 2020","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"Search Engines, GPS Location and Cookies are heavenly technologies.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"Jan 1, 2020 6:00am, after 200 meters of running, my new year resolution came to end with conclusion, I need a new pair of shoes.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"Later, I googled it, then decided to pay a visit to outlet mall looking at latest NIKE, ADIDAS shoes.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"My family took this opportunity to visit Michael Kors, Coach and Hugo Boss. It was huge rush in mall, so I couldn’t decide on any brand.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"(Image: ToiletPaper)","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"By the time, I reached home, Finding a pair of shoe, was declared a national emergency on my phone and heavenly technologies came to my rescue. ","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"All my social media accounts were flooded by MK, Coach, Nike & Adidas ads.  And here comes New Balance, NOBULL, Salomon shoes.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"I finally ordered a new pair of running shoe and a Coach purse. All items came via express shipping in 3 days and so did COVID…","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"Jan 2020, I learned that vaccine is coming soon, Washing hands frequently, use of Hand Sanitizers, keeping 6-ft physical distance and quarantine is only treatment available.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"being a healthcare worker myself, I didn’t panic, At home, I took my research to Google again.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"I searched everything about use of Azithromycin, dextrorphan, levorphanol, DXM, Ramdevpir, Hydroxychloroquine, Virus DNA, impact of virus on people with pre-diabetic, pre-existing conditions etc.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"Meanwhile, Most of hospitals I serve, ran out of surgical gloves, hand sanitizers, face coverings. Cleaning supplies, Chlorine, PerOxide, Bleach disappeared from shelves and toilet paper were sold as gold. Patients visits, health checkups, operational procedures cancelled.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"but, technologies on my phone figured out, I already bought shoes and purse, and now I need joggers.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"Other hand, I was sitting clueless on 35TB of Rx Supply chain data for a decade, ","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"    did we not know already, cleaning supplies like toilet paper, hand sanitizers are essentials?\n    why entire supply chain inventory replenishment process flow collapse?\n    What a doctor will need in her DocX Cart before walking into operation room?\n    Why didn’t we set up an auto-replenishment process based on prediction?\n    how can one stop black marketing and overstocking, specially on life saving essentials?\n    Where are my low inventory items in shipment?","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"I spent last few years figuring out, how to bring predictive analytics to Rx Supply Chain AND Today, ","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"I am here to make this happen.","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"","category":"page"},{"location":"define/","page":"Define Problem","title":"Define Problem","text":"(Image: ToiletPaper)","category":"page"},{"location":"graph/#Graph-Analysis","page":"graph","title":"Graph Analysis","text":"","category":"section"},{"location":"graph/","page":"graph","title":"graph","text":"In previous chapter, we loaded physical ERDs with actual datasets in Julia Dataframes. I also shared, JuliaLang scripts to export these ERP datasets in csv.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"In this chapter, we will create a complete ERP graph with vertices, edges and load these datasets into ERP P2P Graph.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"let's get started.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"","category":"page"},{"location":"graph/#create-graph,-vertices-and-edges","page":"graph","title":"create graph, vertices and edges","text":"","category":"section"},{"location":"graph/","page":"graph","title":"graph","text":"info: Info\nBelow code is an example of using Julia Langauge to call pytigergraph Python package, run GSQLs on TGCLOUD.In future, depending on demand, I may write pytigergprah package in Julia Lang.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"import Pkg\n# you may not need to add conda, pytigergraph\n# if you already have python setup\n# these instructions are specific for julia setup\nPkg.add(\"Conda\")\nENV[\"PYTHON\"] = \"/usr/bin/python3\"\nusing PyCall\nusing Conda\nConda.pip_interop(true;)\n# Conda.pip_interop(true; [env::Environment=\"/usr/bin/python3\"])\nConda.pip(\"install\", \"pyTigerGraph\")\nConda.add(\"pyTigerGraph\")\ntg = pyimport(\"pyTigerGraph\")\n# please don't expect below credentials to work for you, and signup at tgcloud\nhostName = \"https://p2p.i.tgcloud.io\"\nuserName = \"amit\"\npassword = \"password\"\ngraphName = \"P2PFinSCM\"\nconn = tg.TigerGraphConnection(host=hostName, username=userName, password=password, graphname=graphName)\n# conn.gsql(getSchema)","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"note: Note\nBelow code is directly executed over Python environmentfirst you will also need to install pyTigerGraph in your python environment,!pip install -U pyTigerGraphthen execute following commands to create TGCloud Graph","category":"page"},{"location":"graph/#Finance-Graph","page":"graph","title":"Finance Graph","text":"","category":"section"},{"location":"graph/","page":"graph","title":"graph","text":"import pyTigerGraph as tg\nhostName = \"https://p2p.i.tgcloud.io\"\nuserName = \"amit\"\npassword = \"password\"\ngraphName = \"P2PFinSCM\"\nconn = tg.TigerGraphConnection(host=hostName, username=userName, password=password, graphname=graphName)\n\nconn.gsql(\"ls\")\nconn.gsql('''USE GLOBAL\nDROP ALL\n''')\n\nconn.gsql('''\n  USE GLOBAL\n  CREATE VERTEX Account (PRIMARY_ID ID INT, ENTITY STRING, AS_OF_DATE DATETIME, CLASSIFICATION STRING,CATEGORY STRING, STATUS STRING,DESCR STRING,ACCOUNT_TYPE STRING) WITH primary_id_as_attribute=\"true\"\n  CREATE VERTEX Location (PRIMARY_ID ID INT, AS_OF_DATE DATETIME, CLASSIFICATION STRING,CATEGORY STRING, STATUS STRING,DESCR STRING,LOC_TYPE STRING) WITH primary_id_as_attribute=\"true\"\n  CREATE VERTEX Department (PRIMARY_ID ID INT, AS_OF_DATE DATETIME, CLASSIFICATION STRING,CATEGORY STRING, STATUS STRING,DESCR STRING,DEPT_TYPE STRING) WITH primary_id_as_attribute=\"true\"\n  CREATE VERTEX Ledger (PRIMARY_ID LEDGER STRING, FISCAL_YEAR INT, PERIOD INT, ORGID STRING, OPER_UNIT STRING, ACCOUNT INT, DEPT INT, LOCATION INT, POSTED_TOTAL STRING) WITH primary_id_as_attribute=\"true\"\n  CREATE DIRECTED EDGE by_account (From Ledger, To Account) WITH REVERSE_EDGE=\"account_family\"\n  CREATE DIRECTED EDGE by_location (From Ledger, To Location) WITH REVERSE_EDGE=\"region\"\n  CREATE DIRECTED EDGE by_department (From Ledger, To Department) WITH REVERSE_EDGE=\"dept_class\"\n''')\nresults = conn.gsql('CREATE GRAPH P2PFinSCM(Account, Location, Department, Ledger, by_account, by_location, by_department)')","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"(Image: P2P Graph 1)","category":"page"},{"location":"graph/#Loading-Data","page":"graph","title":"Loading Data","text":"","category":"section"},{"location":"graph/","page":"graph","title":"graph","text":"conn.gsql('''\nUSE GLOBAL\nUSE GRAPH P2P\nCREATE LOADING JOB P2P_PATH FOR GRAPH P2P {\nDEFINE FILENAME file1 = \"sampleData/accounts.csv\";\nDEFINE FILENAME file2 = \"sampleData/locations.csv\";\nDEFINE FILENAME file3 = \"sampleData/department.csv\";\nDEFINE FILENAME file4 = \"sampleData/ledger.csv\";\nLOAD file1 TO VERTEX Account VALUES ($\"ENTITY\", $\"AS_OF_DATE\", $\"ID\", $\"CLASSIFICATION\", $\"CATEGORY\", $\"STATUS\", $\"DESCR\", $\"ACCOUNT_TYPE\") USING header=\"true\", separator=\",\";\nLOAD file2 TO VERTEX Location VALUES ($\"AS_OF_DATE\", $\"ID\", $\"CLASSIFICATION\", $\"CATEGORY\", $\"STATUS\", $\"DESCR\", $\"LOC_TYPE\") USING header=\"true\", separator=\",\";\nLOAD file2 TO VERTEX Department VALUES ($\"AS_OF_DATE\", $\"ID\", $\"CLASSIFICATION\", $\"CATEGORY\", $\"STATUS\", $\"DESCR\", $\"DEPT_TYPE\") USING header=\"true\", separator=\",\";\nLOAD file4 TO VERTEX Ledger VALUES ($\"LEDGER\", $\"FISCAL_YEAR\", $\"PERIOD\", $\"ORGID\", $\"OPER_UNIT\", $\"ACCOUNT\", $\"DEPT\", $\"LOCATION\", $\"POSTED_TOTAL\") USING header=\"true\", separator=\",\";\n}\n''')\nresults = conn.gsql('RUN LOADING JOB P2P_PATH USING file1=\"sampleData/galaxy.csv\", \"sampleData/species.csv\", \"sampleData/itemmaster.csv\"')","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"You can also manually upload your data to TGCloud.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"Please see, a copy of datasets can be found inside sampleData folder. or use sampleData jupter notebook to generate more volume data sets.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"(Image: P2P Graph 2)","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"You can also manually map your data to vertices/edges in TGCloud.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"Do NOT forget to publish your data mappings and load data.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"(Image: P2P Graph 3)","category":"page"},{"location":"graph/#Supply-Chain-Graph","page":"graph","title":"Supply Chain Graph","text":"","category":"section"},{"location":"graph/","page":"graph","title":"graph","text":"import pyTigerGraph as tg\nhostName = \"https://p2p.i.tgcloud.io\"\nuserName = \"amit\"\npassword = \"password\"\ngraphName = \"P2PSCM\"\nconn = tg.TigerGraphConnection(host=hostName, username=userName, password=password, graphname=graphName)\n\nconn.gsql(\"ls\")\nconn.gsql('''USE GLOBAL\nDROP ALL\n''')\n\nconn.gsql('''\nUSE GLOBAL\n\nCREATE VERTEX UNSPSC (PRIMARY_ID Code INT, KeyTO STRING, PARENT_KEY STRING, Title STRING) WITH primary_id_as_attribute=\"true\"\nCREATE VERTEX GUDID (PRIMARY_ID PrimaryDI STRING, publicDeviceRecordKey STRING, publicVersionStatus STRING, deviceRecordStatus STRING, publicVersionNumber STRING, publicVersionDate STRING, devicePublishDate STRING, deviceCommDistributionEndDate STRING, deviceCommDistributionStatus STRING, brandName STRING, versionModelNumber STRING, catalogNumber STRING, dunsNumber STRING, companyName STRING, deviceCount STRING, deviceDescription STRING, DMExempt STRING, premarketExempt STRING, deviceHCTP STRING, deviceKit STRING, deviceCombinationProduct STRING, singleUse STRING, lotBatch STRING, serialNumber STRING, manufacturingDate STRING, expirationDate STRING, donationIdNumber STRING, labeledContainsNRL STRING, labeledNoNRL STRING, MRISafetyStatus STRING, rx STRING, otc STRING, deviceSterile STRING, sterilizationPriorToUse STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX LOCATION_MASTER (PRIMARY_ID city STRING, city_ascii STRING, state_id STRING, state_name STRING, county_fips STRING, county_name STRING, lat STRING, lng STRING, population STRING, density STRING, source STRING, military STRING, incorporated STRING, timezone STRING, ranking STRING, zips STRING, id STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX ORG_MASTER (PRIMARY_ID ENTITY STRING, GROUPTOORG STRING, DEPARTMENT STRING, UNIT STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX MSR (PRIMARY_ID UNIT STRING, MSR_DATE STRING, FROM_UNIT STRING, TO_UNIT STRING, GUDID STRING, QTY STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX PO (PRIMARY_ID UNIT STRING, PO_DATE STRING, VENDOR STRING, GUDID STRING, QTY STRING, UNIT_PRICE STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX SALES (PRIMARY_ID UNIT STRING, SALES_DATE STRING, STATUS STRING, SALES_RECEIPT_NUM STRING, CUSTOMER STRING, GUDID STRING, QTY STRING, UNIT_PRICE STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX SHIPRECEIPT (PRIMARY_ID UNIT STRING, SHIP_DATE STRING, STATUS STRING, SHIPMENT_NUM STRING, CUSTOMER STRING, GUDID STRING, QTY STRING, UNIT_PRICE STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX VENDOR (PRIMARY_ID brandName STRING, dunsNumber STRING, companyName STRING, rx STRING, otc STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE VERTEX VOUCHER (PRIMARY_ID UNIT STRING, VCHR_DATE STRING, STATUS STRING, VENDOR_INVOICE_NUM STRING, VENDOR STRING, GUDID STRING, QTY STRING, UNIT_PRICE STRING) WITH primary_id_as_attribute=\"true\"\n\nCREATE DIRECTED EDGE by_UNSPSC (From GUDID, To UNSPSC)\nCREATE DIRECTED EDGE PO_LOCATION_MASTER (From PO, To LOCATION_MASTER)\nCREATE DIRECTED EDGE PO_ORG_MASTER (From PO, To ORG_MASTER)\nCREATE DIRECTED EDGE PO_VENDOR (From PO, To VENDOR)\nCREATE DIRECTED EDGE PO_GUDID (From PO, To GUDID)\nCREATE DIRECTED EDGE MSR_LOCATION_MASTER (From MSR, To LOCATION_MASTER)\nCREATE DIRECTED EDGE MSR_ORG_MASTER (From MSR, To ORG_MASTER)\nCREATE DIRECTED EDGE MSR_VENDOR (From MSR, To VENDOR)\nCREATE DIRECTED EDGE MSR_GUDID (From MSR, To GUDID)\nCREATE DIRECTED EDGE SALES_LOCATION_MASTER (From SALES, To LOCATION_MASTER)\nCREATE DIRECTED EDGE SALES_ORG_MASTER (From SALES, To ORG_MASTER)\nCREATE DIRECTED EDGE SALES_VENDOR (From SALES, To VENDOR)\nCREATE DIRECTED EDGE SALES_GUDID (From SALES, To GUDID)\nCREATE DIRECTED EDGE SHIPRECEIPT_LOCATION_MASTER (From SHIPRECEIPT, To LOCATION_MASTER)\nCREATE DIRECTED EDGE SHIPRECEIPT_ORG_MASTER (From SHIPRECEIPT, To ORG_MASTER)\nCREATE DIRECTED EDGE SHIPRECEIPT_VENDOR (From SHIPRECEIPT, To VENDOR)\nCREATE DIRECTED EDGE SHIPRECEIPT_GUDID (From SHIPRECEIPT, To GUDID)\nCREATE DIRECTED EDGE VOUCHER_LOCATION_MASTER (From VOUCHER, To LOCATION_MASTER)\nCREATE DIRECTED EDGE VOUCHER_ORG_MASTER (From VOUCHER, To ORG_MASTER)\nCREATE DIRECTED EDGE VOUCHER_VENDOR (From VOUCHER, To VENDOR)\nCREATE DIRECTED EDGE VOUCHER_GUDID (From VOUCHER, To GUDID)\n''')\nresults = conn.gsql('''CREATE GRAPH P2PSCM(UNSPSC, GUDID, LOCATION_MASTER, ORG_MASTER, MSR, PO, SALES, SHIPRECEIPT, VENDOR, VOUCHER, \nby_UNSPSC, PO_LOCATION_MASTER, PO_ORG_MASTER, PO_VENDOR, PO_GUDID, MSR_LOCATION_MASTER, MSR_ORG_MASTER, MSR_VENDOR, MSR_GUDID, SALES_LOCATION_MASTER, SALES_ORG_MASTER, SALES_VENDOR, SALES_GUDID, SHIPRECEIPT_LOCATION_MASTER, SHIPRECEIPT_ORG_MASTER, SHIPRECEIPT_VENDOR, SHIPRECEIPT_GUDID, VOUCHER_LOCATION_MASTER, VOUCHER_ORG_MASTER, VOUCHER_VENDOR, VOUCHER_GUDID)''')","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"Do NOT forget to publish your data mappings and load data.","category":"page"},{"location":"graph/","page":"graph","title":"graph","text":"(Image: P2P Graph 4)","category":"page"},{"location":"analytics/#Analytics","page":"analytics","title":"Analytics","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"In previous chapters, we learned, created and loaded Finance and Supply chain datasets in to Julia language dataframes, RDBMS database.","category":"page"},{"location":"analytics/","page":"analytics","title":"analytics","text":"We also created a Graph schema, loaded ERP data into Graph schema and created pattern matching Graph SQLs to get data insights.","category":"page"},{"location":"analytics/","page":"analytics","title":"analytics","text":"In this chapter, we'll load Original ERP dataset WITH Graph GSQLs results to run interactive data analytics.","category":"page"},{"location":"analytics/","page":"analytics","title":"analytics","text":"These interactive data anlytics will help us run interactive visualiztion/analysis on following sample scenarios.","category":"page"},{"location":"analytics/#what-if,-would,-could,-should","page":"analytics","title":"what-if, would, could, should","text":"","category":"section"},{"location":"analytics/#Finance","page":"analytics","title":"Finance","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"Region A is merged with Region B\nEmployee resume work from office, how much Travel amounts % will increase.\n% of Office supply expenses given to Employee as home office setup\nwould Region A, Cash Flow Investment have returned 7% ROI\nwould Region B received Government/investor funding\ncould have increased IT operating expenses by 5%\ncould have reduced HR temp staff\nshould have paid vendor invoiced on time to recive rebate\nshould have applied loan to increase production\nshould have retired a particular Asset","category":"page"},{"location":"analytics/#Supply-chain","page":"analytics","title":"Supply chain","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"during 2020, what UNSPSC category experience highest surge in source orders\nduring 2020, in \"cleaning, supplies\" category, what are the other items sold more than average\nwhat are most requested items in \"cleaning, supplies\" category\nwhat are most requested items in \"essentials\" category\nwhat are most requested items in \"life savings drugs / GUDID implants\" category\nwas there an decrease in implants, in other words, were they any cardiac or other critical operational surgery delay during covid\nwhat are items purchase together with devices in category = ventilators\nwas there an increased use items categorized as \"controlled substance\" items\nwhat are the items, which were in backlog most frequently and waited longest","category":"page"},{"location":"analytics/#GL-BalanceSheet,-IncomeStatement-and-CashFlow","page":"analytics","title":"GL BalanceSheet, IncomeStatement & CashFlow","text":"","category":"section"},{"location":"analytics/#Balance-Sheet-(Interactive)","page":"analytics","title":"Balance Sheet (Interactive)","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"@manipulate for ld = Dict(\"Actuals\"=> \"Actuals\", \"Budget\" => \"Budget\"), \n                rg = Dict(\"Region A\"=> \"Region A\", \"Region B\" => \"Region B\", \"Region C\" => \"Region C\"),\n                yr = slider(2020:1:2022; value=2021),\n                qtr = 1:1:4\n    \n    @show ld, rg, yr, qtr\n    \nselect(gdf_plot[(\n    (gdf_plot.FISCAL_YEAR .== yr)\n    .&\n    (gdf_plot.QTR .== qtr)\n    .&\n    (gdf_plot.LEDGER .== ld)\n    .&\n    (gdf_plot.OPER_UNIT .== rg)\n    ),:],\n        :OPER_UNIT => :Org,\n        :FISCAL_YEAR => :FY,\n        :QTR => :Qtr,\n        :ACCOUNTS_CLASSIFICATION => :Accounts,\n        :DEPT_CLASSIFICATION => :Dept,\n        # :LOCATION_CLASSIFICATION => :Region,\n        :LOCATION_DESCR => :Loc,\n        :TOTAL => :TOTAL)\nend","category":"page"},{"location":"analytics/","page":"analytics","title":"analytics","text":"using Pkg\nPkg.add(\"DataFrames\")\nPkg.add(\"Dates\")\nPkg.add(\"CategoricalArrays\")\nPkg.add(\"Interact\")\nPkg.add(\"WebIO\")\nPkg.build(\"WebIO\")\nusing DataFrames, Dates, Interact, CategoricalArrays, WebIO\nPkg.status();","category":"page"},{"location":"analytics/","page":"analytics","title":"analytics","text":"using DataFrames, Dates\n# create dummy data\naccountsDF = DataFrame(\n    ENTITY = \"Apple Inc.\",\n    AS_OF_DATE=Date(\"1900-01-01\", dateformat\"y-m-d\"),\n    ID = 11000:1000:45000,\n    CLASSIFICATION=repeat([\n        \"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=5),\n    CATEGORY=[\n        \"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n        \"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n        \"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n        \"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n        \"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n        \"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n        \"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"],\n    STATUS=\"A\",\n    DESCR=repeat([\n    \"operating expenses\",\"non-operating expenses\",\"assets\",\"liability\",\"net-worth\",\"stats\",\"revenue\"], inner=5),\n    ACCOUNT_TYPE=repeat([\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"],inner=5));\n\n# DEPARTMENT Chartfield\ndeptDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 1100:100:1500,\n    CLASSIFICATION=[\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"],\n    CATEGORY=[\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"],\n    STATUS=\"A\",\n    DESCR=[\n    \"Sales & Marketing\",\"Human Resource\",\"Infomration Technology\",\"Business leaders\",\"other temp\"\n        ],\n    DEPT_TYPE=[\"S\",\"H\",\"I\",\"B\",\"O\"]);\n\n# LOCATION Chartfield\nlocationDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 11:1:22,\n    CLASSIFICATION=repeat([\n        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n    CATEGORY=repeat([\n        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n    STATUS=\"A\",\n    DESCR=[\n\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n\"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n\"Dallas\",\"San Francisco\"],\n    LOC_TYPE=\"Physical\");\n\n# creating Ledger\nledgerDF = DataFrame(\n            LEDGER = String[], FISCAL_YEAR = Int[], PERIOD = Int[], ORGID = String[],\n            OPER_UNIT = String[], ACCOUNT = Int[], DEPT = Int[], LOCATION = Int[],\n            POSTED_TOTAL = Float64[]\n            );\n\n# create 2020 Period 1-12 Actuals Ledger \nl = \"Actuals\";\nfy = 2020;\nfor p = 1:12\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# create 2021 Period 1-4 Actuals Ledger \nl = \"Actuals\";\nfy = 2021;\nfor p = 1:4\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# create 2021 Period 1-4 Budget Ledger \nl = \"Budget\";\nfy = 2021;\nfor p = 1:12\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# here is ~3 million rows ledger dataframe\nsize(ledgerDF)\n\n# rename dimensions columns for innerjoin\ndf_accounts = rename(accountsDF, :ID => :ACCOUNTS_ID, :CLASSIFICATION => :ACCOUNTS_CLASSIFICATION, \n    :CATEGORY => :ACCOUNTS_CATEGORY, :DESCR => :ACCOUNTS_DESCR);\ndf_dept = rename(deptDF, :ID => :DEPT_ID, :CLASSIFICATION => :DEPT_CLASSIFICATION, \n    :CATEGORY => :DEPT_CATEGORY, :DESCR => :DEPT_DESCR);\ndf_location = rename(locationDF, :ID => :LOCATION_ID, :CLASSIFICATION => :LOCATION_CLASSIFICATION,\n    :CATEGORY => :LOCATION_CATEGORY, :DESCR => :LOCATION_DESCR);\n\n# join Ledger accounts chartfield with accounts chartfield dataframe to pull all accounts fields\n# join Ledger dept chartfield with dept chartfield dataframe to pull all dept fields\n# join Ledger location chartfield with location chartfield dataframe to pull all location fields\ndf_ledger = innerjoin(\n                innerjoin(\n                    innerjoin(ledgerDF, df_accounts, on = [:ACCOUNT => :ACCOUNTS_ID], makeunique=true),\n                    df_dept, on = [:DEPT => :DEPT_ID], makeunique=true), df_location,\n                on = [:LOCATION => :LOCATION_ID], makeunique=true);\n\n# note, how ledger DF has 28 columns now (inclusive of all chartfields join)\nsize(df_accounts),size(df_dept),size(df_location), size(ledgerDF), size(df_ledger)\n\nfunction periodToQtr(x)\n    if x ∈ 1:3\n        return 1\n    elseif x ∈ 4:6\n        return 2\n    elseif x ∈ 7:9\n        return 3\n    else return 4\n    end\nend\n\n# now we will use this function to transform a new column\ntransform!(df_ledger, :PERIOD => ByRow(periodToQtr) => :QTR)\n\n# let's create one more generic function, which converts a number to USD currency\nfunction numToCurrency(x)\n        return string(\"USD \",round(x/10^6; digits = 2), \" million\")\nend\n\ntransform!(df_ledger, :POSTED_TOTAL => ByRow(numToCurrency) => :TOTAL)\ndf_ledger[1:5,[\"POSTED_TOTAL\",\"TOTAL\"]]\n\"df_ledger_size after transformation is: \", size(df_ledger)","category":"page"},{"location":"analytics/#Income-Statement-(Interactive)","page":"analytics","title":"Income Statement (Interactive)","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"@manipulate for ld = Dict(\"Actuals\"=> \"Actuals\", \"Budget\" => \"Budget\"), \n                rg = Dict(\"Region A\"=> \"Region A\", \"Region B\" => \"Region B\", \"Region C\" => \"Region C\"),\n                yr = slider(2020:1:2022; value=2021),\n                qtr = 1:1:4\n    \n    @show ld, rg, yr, qtr\n    \nselect(gdf_plot[(\n    (gdf_plot.FISCAL_YEAR .== yr)\n    .&\n    (gdf_plot.QTR .== qtr)\n    .&\n    (gdf_plot.LEDGER .== ld)\n    .&\n    (gdf_plot.OPER_UNIT .== rg)\n    .&\n    (in.(gdf_plot.ACCOUNTS_CLASSIFICATION, Ref([\"ASSETS\", \"LIABILITIES\", \"REVENUE\",\"NET_WORTH\"])))\n    ),:],\n        :OPER_UNIT => :Org,\n        :FISCAL_YEAR => :FY,\n        :QTR => :Qtr,\n        :ACCOUNTS_CLASSIFICATION => :Accounts,\n        :DEPT_CLASSIFICATION => :Dept,\n        # :LOCATION_CLASSIFICATION => :Region,\n        :LOCATION_DESCR => :Loc,\n        :TOTAL => :TOTAL)\nend","category":"page"},{"location":"analytics/#Cash-Flow-Statement-(Interactive)","page":"analytics","title":"Cash Flow Statement (Interactive)","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"@manipulate for ld = Dict(\"Actuals\"=> \"Actuals\", \"Budget\" => \"Budget\"), \n                rg = Dict(\"Region A\"=> \"Region A\", \"Region B\" => \"Region B\", \"Region C\" => \"Region C\"),\n                yr = slider(2020:1:2022; value=2021),\n                qtr = 1:1:4\n    \n    @show ld, rg, yr, qtr\n    \nselect(gdf_plot[(\n    (gdf_plot.FISCAL_YEAR .== yr)\n    .&\n    (gdf_plot.QTR .== qtr)\n    .&\n    (gdf_plot.LEDGER .== ld)\n    .&\n    (gdf_plot.OPER_UNIT .== rg)\n    .&\n    (in.(gdf_plot.ACCOUNTS_CLASSIFICATION, Ref([\"NON-OPERATING_EXPENSES\",\"OPERATING_EXPENSES\"])))\n    ),:],\n        :OPER_UNIT => :Org,\n        :FISCAL_YEAR => :FY,\n        :QTR => :Qtr,\n        :ACCOUNTS_CLASSIFICATION => :Accounts,\n        :DEPT_CLASSIFICATION => :Dept,\n        # :LOCATION_CLASSIFICATION => :Region,\n        :LOCATION_DESCR => :Loc,\n        :TOTAL => :TOTAL)\nend","category":"page"},{"location":"analytics/#Supplychain-Material-Service-Request","page":"analytics","title":"Supplychain Material Service Request","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"sampleSize = 1000 # number of rows, scale as needed\n\ndfMSR = DataFrame(\n    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n    MSR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n    FROM_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n    TO_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n    QTY = rand(dfOrgMaster.UNIT, sampleSize));\nfirst(dfMSR, 5)","category":"page"},{"location":"analytics/#ITEM/Vendor-Master-Voucher-Invoice-data","page":"analytics","title":"ITEM/Vendor Master Voucher Invoice data","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"sampleSize = 1000 # number of rows, scale as needed\n\ndfVCHR = DataFrame(\n    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n    VCHR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n    STATUS=rand([\"Closed\",\"Paid\",\"Open\",\"Cancelled\",\"Exception\"], sampleSize),\n    VENDOR_INVOICE_NUM = rand(10001:9999999, sampleSize),\n    VENDOR=rand(unique(dfVendor.companyName), sampleSize),\n    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n    QTY = rand(1:150, sampleSize),\n    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n    );\nshow(first(dfVCHR, 5),allcols=true)","category":"page"},{"location":"analytics/#Supply-chain-Shipment-Receipt-data","page":"analytics","title":"Supply chain Shipment Receipt data","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"sampleSize = 1000 # number of rows, scale as needed\ndfSHIPRECEIPT = DataFrame(\n    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n    SHIP_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n    STATUS=rand([\"Shippped\",\"Returned\",\"In process\",\"Cancelled\",\"Exception\"], sampleSize),\n    SHIPMENT_NUM = rand(10001:9999999, sampleSize),\n    CUSTOMER=rand(unique(dfVendor.companyName), sampleSize),\n    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n    QTY = rand(1:150, sampleSize),\n    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n    );\nshow(first(dfSHIPRECEIPT, 5),allcols=true)","category":"page"},{"location":"analytics/#Supply-chain-Procurement-Pipeline","page":"analytics","title":"Supply chain Procurement Pipeline","text":"","category":"section"},{"location":"analytics/","page":"analytics","title":"analytics","text":"## Ledger Visual\n#\tplot_data = gdf_plot[(\n#\t\t(gdf_plot.FISCAL_YEAR .== yr_p)\n#\t\t.&\n#\t\t(gdf_plot.LEDGER .== ld_p)\n#\t\t.&\n#\t\t(gdf_plot.OPER_UNIT .== rg_p)\n#\t\t.&\n#\t\t(gdf_plot.LOCATION_DESCR .== ldescr)\n#\t\t.&\n#\t\t(gdf_plot.DEPT_CLASSIFICATION .== ddescr)\n#\t\t.&\n#\t\t(gdf_plot.ACCOUNTS_CLASSIFICATION .== adescr))\n#\t\t, :];\n#\t# @df plot_data scatter(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", label=\"$ld_p Total by #    $yr_p for $rg_p\")\n#\t@df plot_data plot(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", \n#\t\tlabel=[\n#\t\t\t\"$ld_p by $yr_p for $rg_p $ldescr $adescr $ddescr\"\n#\t\t\t],\n#\t\tlw=3)\n\n## Actuals vs Budget comparison\n\n#\tplot_data_a = gdf_plot[(\n#\t\t(gdf_plot.FISCAL_YEAR .== yr_p)\n#\t\t.&\n#\t\t(gdf_plot.LEDGER .== \"Actuals\")\n#\t\t.&\n#\t\t(gdf_plot.OPER_UNIT .== rg_p)\n#\t\t.&\n#\t\t(gdf_plot.LOCATION_DESCR .== ldescr)\n#\t\t.&\n#\t\t(gdf_plot.DEPT_CLASSIFICATION .== ddescr)\n#\t\t.&\n#\t\t(gdf_plot.ACCOUNTS_CLASSIFICATION .== adescr))\n#\t\t, :];\n\t# @df plot_data scatter(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", label=\"$ld_p Total by # $yr_p for $rg_p\")\n#\tplot_data_b = gdf_plot[(\n#\t\t(gdf_plot.FISCAL_YEAR .== yr_p)\n#\t\t.&\n#\t\t(gdf_plot.LEDGER .== \"Budget\")\n#\t\t.&\n#\t\t(gdf_plot.OPER_UNIT .== rg_p)\n#\t\t.&\n#\t\t(gdf_plot.LOCATION_DESCR .== ldescr)\n#\t\t.&\n#\t\t(gdf_plot.DEPT_CLASSIFICATION .== ddescr)\n#\t\t.&\n#\t\t(gdf_plot.ACCOUNTS_CLASSIFICATION .== adescr))\n#\t\t, :];\n# @df plot_data scatter(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", label=\"$ld_p Total by $yr_p # for $rg_p\")\n#\t@df plot_data_a plot(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", \n#\t\tlabel=[\n#\t\t\t\"Actuals by $yr_p for $rg_p $ldescr $adescr $ddescr\"\n#\t\t\t],\n#\t\tlw=3)\n#\t@df plot_data_b plot!(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", \n#\t\tlabel=[\n#\t\t\t\"Budget by $yr_p for $rg_p $ldescr $adescr $ddescr\"\n#\t\t\t],\n#\t\tlw=3)\n\n# plot_data\n\n\t# plot_data = gdf_plot[(\n\t# \t(gdf_plot.FISCAL_YEAR .== yr_p)\n\t# \t.&\n\t# \t(gdf_plot.LEDGER .== ld_p)\n\t# \t.&\n\t# \t(gdf_plot.OPER_UNIT .== rg_p)\n\t# \t.&\n\t# \t(gdf_plot.LOCATION_DESCR .== ldescr)\n\t# \t.&\n\t# \t(gdf_plot.DEPT_CLASSIFICATION .== ddescr)\n\t# \t.&\n\t# \t(gdf_plot.ACCOUNTS_CLASSIFICATION .== adescr))\n\t# \t, :];\n\t# @df plot_data scatter(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", label=\"$ld_p Total by $yr_p for $rg_p\")\n\t# @df gdf_plot plot(:QTR, :ACCOUNTS_CLASSIFICATION, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", \n\t# \tlabel=[\n\t# \t\t\"$ld_p by $yr_p for $rg_p $ldescr $adescr $ddescr\"\n\t# \t\t],\n\t# \tlw=3)\n#\t@df gdf_plot scatter(:QTR, :ACCOUNTS_CLASSIFICATION, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", \n#\t\tlabel=[\n#\t\t\t\"$ld_p by $yr_p for $rg_p $ldescr for $ddescr\"\n#\t\t\t],\n#\t\tlw=3)\n\n\n\t# plot_data = gdf_plot[(\n\t# \t(gdf_plot.FISCAL_YEAR .== yr_p)\n\t# \t.&\n\t# \t(gdf_plot.LEDGER .== ld_p)\n\t# \t.&\n\t# \t(gdf_plot.OPER_UNIT .== rg_p)\n\t# \t.&\n\t# \t(gdf_plot.LOCATION_DESCR .== ldescr)\n\t# \t.&\n\t# \t(gdf_plot.DEPT_CLASSIFICATION .== ddescr)\n\t# \t.&\n\t# \t(gdf_plot.ACCOUNTS_CLASSIFICATION .== adescr))\n\t# \t, :];\n\t# @df plot_data scatter(:QTR, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", label=\"$ld_p Total by $yr_p for $rg_p\")\n\t# @df gdf_plot plot(:QTR, :ACCOUNTS_CLASSIFICATION, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", \n\t# \tlabel=[\n\t# \t\t\"$ld_p by $yr_p for $rg_p $ldescr $adescr $ddescr\"\n\t# \t\t],\n\t# \tlw=3)\n#\t@df gdf_plot scatter(:QTR, :DEPT_CLASSIFICATION, :TOTAL/10^8, title = \"Finance Ledger Data\", xlabel=\"Quarter\", ylabel=\"Total (in USD million)\", \n#\t\tlabel=[\n#\t\t\t\"$ld_p by $yr_p for $rg_p $ldescr for $adescr\"\n#\t\t\t],\n#\t\tlw=3)","category":"page"},{"location":"path/#How-did-we-get-here","page":"How did we get here","title":"How did we get here","text":"","category":"section"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"well.. let's start from the very beginning,  and I'll try to add some humour, will make silly mistakes, leave out some details and only focus on topics which interests me to come to point quickly.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"almost four and half billion years ago, some one created a planet name earth 🌎 or other theory is, perhaps gravity pulled swirling gas 🌀 and dust ☄️ in to become the third planet from the Sun 🌞.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"like, I mentioned earlier, let's leave out the details how it was formed and deal with Fact, earth 🌎 is here to stay for quite some more time.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"soon, with water, air, fire, sky, earth became home to things which started crawling, swimming, walking and running all over it, let's call these living things species.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"Most of these species have one common pattern in behavior. They all eat 🥣, drink 🍷, sleep 🛌 and every time this cycle is broken, some species survive and other die 🪦.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"for example, Dinosaurs 🦖, mammoths 🦣 and big foot 🦶🏿 couldn't survive because they cycle was interrupted. It's not, these species don't fight back with interruptions, they do, and when successful, they survive. Like cockroaches 🪳, they lived before Dinosaurs 🦖 and still here living healthy.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"Most recent latest versions of one such species name human 🧝, has one attribute missing called intelligence 🧐💡 which all other species ever had.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"which concludes to my point, let's focus on one such species name human, it's behavior pattern i.e. eat 🥣, drink 🍷, sleep 🛌 and missing attribute called intelligence 🧐💡.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"let's visualize these points to paint our big picture.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"# we are using Julia Language for Graph analysis\n# TigerGraph provide RESTAPI end points, GSQL and GRAPHSTUDIO to connect TIGERGRAPH\n# pyTigerGraph is a Python based library to connect with GRAPH database and run GSQLs\n# we will use Julia PyCall package to connect with pyTigerGraph library\n#######################################################################\n## **perhaps, some day I will re-write pyTigerGraph package in Julia ##\n#######################################################################\n\n# open Julia REPL, Jupyter or your favorite Julia IDE, run following\n\n# first import all packages required to support our data analysis\n# rest of this chapter assume that below packages are imported once\nimport Pkg\nPkg.add(\"DataFrames\")\nPkg.add(\"CSV\")\nPkg.add(\"PyCall\")\nPkg.build(\"PyCall\");\n\n# you will also need to install pyTigerGraph in your python environment\n# !pip install -U pyTigerGraph","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"info: Info\nbefore proceeding any further, please setup Tiger Graph Server instance at tgcloud.io please don't expect these credentials to work for you, as there is cost involved to keep this.hostName = \"https://p2p.i.tgcloud.io\"userName = \"tigercloud\"password = \"tigercloud\"conn = tg.TigerGraphConnection(host=hostName, username=userName, password=password)","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"now once you have TigerGraph and Julia environments setup, let's jump on to setup sample graph, vertices and edges to get a hang of tools.","category":"page"},{"location":"path/#create-graph,-vertices-and-edges","page":"How did we get here","title":"create graph, vertices and edges","text":"","category":"section"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"info: Info\nBelow code is an example of using Julia Langauge to call pytigergraph Python package, run GSQLs on TGCLOUD.In future, depending on demand, I may write pytigergprah package in Julia Lang.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"import Pkg\n# you may not need to add conda, pytigergraph\n# if you already have python setup\n# these instructions are specific for julia setup\nPkg.add(\"Conda\")\nENV[\"PYTHON\"] = \"/usr/bin/python3\"\nusing PyCall\nusing Conda\nConda.pip_interop(true;)\n# Conda.pip_interop(true; [env::Environment=\"/usr/bin/python3\"])\nConda.pip(\"install\", \"pyTigerGraph\")\nConda.add(\"pyTigerGraph\")\ntg = pyimport(\"pyTigerGraph\")\n# please don't expect below credentials to work for you, and signup at tgcloud\nhostName = \"https://p2p.i.tgcloud.io\"\nuserName = \"amit\"\npassword = \"password\"\ngraphName = \"P2P\"\nconn = tg.TigerGraphConnection(host=hostName, username=userName, password=password, graphname=graphName)\n# conn.gsql(getSchema)","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"warning: Warning\nOperations that DO NOT need a TokenViewing the schema of your graph using functions such as getSchema and getVertexTypes does not require you to have an authentication token. A token is also not required to run gsql commands through pyTigerGraph.Sample Connectionconn = tg.TigerGraphConnection(host='https://pytigergraph-demo.i.tgcloud.io', username='tigergraph' password='password' graphname='DemoGraph')Operations that DO need a TokenA token is required to view or modify any actual DATA in the graph. Examples are: upserting data, deleting edges, and getting stats about any loaded vertices. A token is also required to get version data about the TigerGraph instance.Sample Connectionconn = tg.TigerGraphConnection(host='https://pytigergraph-demo.i.tgcloud.io', username='tigergraph' password='password' graphname='DemoGraph', apiToken='av1im8nd2v06clbnb424jj7fp09hp049')","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"note: Note\nBelow code is directly executed over Python environmentfirst you will also need to install pyTigerGraph in your python environment,!pip install -U pyTigerGraphthen execute following commands to create TGCloud Graph","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"import pyTigerGraph as tg\nhostName = \"https://p2p.i.tgcloud.io\"\nuserName = \"amit\"\npassword = \"password\"\ngraphName = \"P2P\"\nconn = tg.TigerGraphConnection(host=hostName, username=userName, password=password, graphname=graphName)\n\nconn.gsql(\"ls\")\nconn.gsql('''USE GLOBAL\nDROP ALL\n''')\n\nconn.gsql('''\n  USE GLOBAL\n  CREATE VERTEX Galaxy (PRIMARY_ID unit STRING, star BOOL, gas BOOL, dust BOOL, breathableAir BOOL, water BOOL, land BOOL, sky BOOL) WITH primary_id_as_attribute=\"true\"\n  CREATE VERTEX Species (PRIMARY_ID unit STRING, intelligence BOOL, crawl INT, swim INT, walk INT, runningSpeed INT, eat INT, drink INT, sleep INT) WITH primary_id_as_attribute=\"true\"\n  CREATE VERTEX ItemMaster (PRIMARY_ID unit STRING, drug BOOL, category STRING, essentials BOOL, luxury BOOL) WITH primary_id_as_attribute=\"true\"\n  CREATE DIRECTED EDGE live_in (From Species, To Galaxy, living_since DATETIME) WITH REVERSE_EDGE=\"is_home\"\n  CREATE UNDIRECTED EDGE behavior (From Species, To Species, like_date DATETIME)\n  CREATE DIRECTED EDGE give_food (From Galaxy, To Species) WITH REVERSE_EDGE=\"feeds\"\n  CREATE DIRECTED EDGE essentials (From Species, To Species) WITH REVERSE_EDGE=\"survival_Kit\"\n  CREATE DIRECTED EDGE used_by (From ItemMaster, To Species)\n''')\nresults = conn.gsql('CREATE GRAPH P2P(Galaxy, Species, ItemMaster, live_in, behavior, give_food, essentials, used_by)')","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"(Image: Graph 1)","category":"page"},{"location":"path/#Loading-Data","page":"How did we get here","title":"Loading Data","text":"","category":"section"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"conn.gsql('''\nUSE GLOBAL\nUSE GRAPH P2P\nCREATE LOADING JOB P2P_PATH FOR GRAPH P2P {\nDEFINE FILENAME file1 = \"sampleData/galaxy.csv\";\nDEFINE FILENAME file2 = \"sampleData/species.csv\";\nDEFINE FILENAME file3 = \"sampleData/itemmaster.csv\";\nLOAD file1 TO VERTEX Galaxy VALUES ($\"unit\", $\"star\", $\"gas\", $\"dust\", $\"breathableAir\", $\"water\", $\"land\", $\"sky\") USING header=\"true\", separator=\",\";\nLOAD file2 TO VERTEX Species VALUES ($\"unit\", $\"intelligence\", $\"crawl\", $\"swim\", $\"walk\", $\"runningSpeed\", $\"eat\", $\"drink\", $\"sleep\") USING header=\"true\", separator=\",\";\nLOAD file3 TO VERTEX ItemMaster VALUES ($\"unit\", $\"drug\", $\"category\", $\"essentials\", $\"luxury\") USING header=\"true\", separator=\",\";\n}\n''')\n\nresults = conn.gsql('RUN LOADING JOB P2P_PATH USING file1=\"sampleData/galaxy.csv\", \"sampleData/species.csv\", \"sampleData/itemmaster.csv\"')","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"You can also manually upload your data to TGCloud.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"Please see, a copy of datasets can be found inside sampleData folder. or use sampleData jupter notebook to generate more volume data sets.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"(Image: Graph 2)","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"You can also manually map your data to vertices/edges in TGCloud.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"(Image: Graph 3)","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"** Do NOT forget to publish your data mappings and load data.**","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"(Image: Graph 4)","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"Conclusion","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"I would love to share my queries with you in details (and you'll figure it out, that I cheated). but here is the conclusion.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"Human needs TP, Cleaning Supplies as much as \"Morphine\" or \"Oxygen\" and god forbid, perhaps \"Ventilators\".","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"But Cockroaches don't because Cockroaches are living before Dinosaurs and still around, and that is all because of their behaviors which helps them survive.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"which is Eat Less, Drink more, Sleep less and RUN all day long.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"(Image: Graph 5)","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"I am not sure, if you buy all the logic above or not, but I am sure, you now know how to create, plot and define graph, vertices and edges.","category":"page"},{"location":"path/","page":"How did we get here","title":"How did we get here","text":"In next chapter, I promise, I will not use silly logic and instead will focus on real life data and use cases.","category":"page"},{"location":"ml/#Machine-Learning","page":"AI predictions","title":"Machine Learning","text":"","category":"section"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"NoteBook  predict item surges","category":"page"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"NoteBook  items bought together","category":"page"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"NoteBook  predict extended delayed item categories","category":"page"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"NoteBook  predict extended delayed UNSPSC family","category":"page"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"NoteBook  predict increase material service request by region","category":"page"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"NoteBook  predict turn-around time by item i.e. procurement pipeline","category":"page"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"","category":"page"},{"location":"ml/#Real-time-TimeSeries,-StatsModel-predictions","page":"AI predictions","title":"Real-time TimeSeries, StatsModel predictions","text":"","category":"section"},{"location":"ml/","page":"AI predictions","title":"AI predictions","text":"Predict Operating and non-operating expense for year\nPredict Actuals to Budget variance and FORECAST\nusing SARIMA model to predict \"Region A\" NET-WORTH","category":"page"},{"location":"facts/#Getting-Facts-data","page":"get Facts straight","title":"Getting Facts data","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"info: ERP Data Structure\nIn this chapter, we will read, write and understand typical Finance, Supply chain datasets in Julia language. These datasets will layout foundations for Finance, Supply Chain Anaytics.Target Audience: This chapter, is meant for Julia Language consultants, ERP Analysts, IT Developers, Finance, Supply chain, HR & CRM managers, executive leaders or anyone curious to implement data science concepts in ERP space.Once we understand these datasets, in following chapters, we will create Graphs, Vertices and Edges for data analysis, load actual data into Graphs.P2P.jl package supports these ERP systems data structures.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"most of the time, Organizations store these datasets in RDBMS tables, document databases and in some case, actual PDFs, images serve as document data itself.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"In previous chapter, we saw business process operational workflow diagram and also learned high level physical ERDs.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"In this chapter, we will use Julia language and Julia packages to mimic Finance & Supply chain data. Majority of this data is sample data and DOES NOT  bear any resemblance to real life data.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"at the same time, I am using lots of actual real life data like UNSPSC codes, GUDID, Vendor, Item master data. Also, These datasets are not specific to any particular ERP systems like SAP, Oracle etc.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"However, these sample dataset below are very close to real life data sets and are great assets to learn ERP systems architecture.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"let's get started.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Background: Most of Enterprise ERP providers like SAP, Oracle, Microsoft build HCM, Finance, Supply Chain, CRM like systems, which store data in highly structured RDBMS tables. Recent advancements in ERP systems also support authoring non-structured data like digital invoices, receipt or hand-held OCR readers.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"All of these ERP systems are great OLTP systems, but depend on Analytic systems for creating dashboards, ad-hoc analysis, operational reporting or live predictive analytics.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Further, ERP systems depend on ELT/ELT or 3rd party tools for data mining, analysis and visualizations.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"While data engineers use Java, Scala, SPARK based big data solutions to move data, they depend on 3rd party BI Reporting tools for creating dashboards, use Data Mining tools for data cleansing and AI Languages for advance predictive analytics.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"When I started learning more about Julia Language, I thought of using Julia Language to solve ERP Analytics multiple languages problem. Why not just use Julia Language to move, clean massive data set as Big data reporting solution, as Julia support multi-threading, distributing parallel computing. Julia language and associated packages has first class support for large arrays, which can be used for data analysis.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"and Julia has great visualization packages to publish interactive dashboards, live data reporting.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"best of all, Julia is great in numerical computing, advance data science machine learning.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"This blog, I am sharing my notes specific to perform typical ERP data analysis using Julia Language.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"","category":"page"},{"location":"facts/#About-ERP-Systems,-General-Ledger-and-Supply-chain","page":"get Facts straight","title":"- About ERP Systems, General Ledger & Supply chain","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"A typical ERP system consists of many modules based on business domain, functions and operations. GL is core of Finance and Supply chain domains and Buy to Pay, Order to Cash deal with different aspects of business operations in an Organization. Many organization, use ERPs in different ways and may chose to implement all or some of the modules. You can find examples of module specific business operations/processes diagram here.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"General Ledger process flow\nAccount Payable process flow\nTax Analytics\nSample GL ERD - Entity Relaton Diagram","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"A typical ERP modules list looks like below diagram.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"(Image: ERP Modules)","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"A typical ERP business process flow looks like below diagram.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"(Image: ERP Processes)","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"A typical GL Balance sheet, Cash-flow or Income Statement looks like this ","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"click here","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"In this notebook, I will do my best to cite examples from real world data like above mentioned GL Financial statement.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"","category":"page"},{"location":"facts/#start-with-Julia","page":"get Facts straight","title":"start with Julia","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"It literally takes < 1 min to install Julia environments on almost any machine.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Here is link to my tutorial, which discuss Julia installation on different machines (including remote and mobile tablets).","category":"page"},{"location":"facts/#adding-Packages","page":"get Facts straight","title":"adding Packages","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using Pkg\nPkg.add(\"DataFrames\")\nPkg.add(\"Dates\")\nPkg.add(\"CategoricalArrays\")\nusing DataFrames, Dates, CategoricalArrays\nPkg.status()","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"rest of this blog, I will assume, all packages are added and imported in current namespace/notebook scope.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"# run one command at a time\nrepeat([\"AMIT\",\"SHUKLA\"], inner=5) # repeat list/string number of times\nfill(\"34\", 4) # repeat list/string number of times\nrange(1.0, stop=9.0, length=100) # generate n number of equal values between start and stop values\n11000:1000:45000 # genarate a range of # from start to finish with set intervals\ncollect(1:4) # collect funtion collect all values in list\nrand([1,2,3,4]) # random value from a list of values\nrand(11000:1000:45000) # random value from a list of values\nrandn() # random # from a list of float values (+ or -)","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, CategoricalArrays\n# run one command at a time\n# basic dataframe is constructed by passing column vectors (think of adding one excel column at a time)\norg = \"Apple Inc\" # is a simple string\n_ap = [1,2] # this is a vector\n\nap = categorical(_ap) # this is a vector\nfy = categorical(repeat([2022], inner=2)) # this is a vector\n\nactuals, budget = (98.40, 100) # this is a tuple\namount = (actuals = 98.54, budget = 100) # this is a named tuple\ndf_Ledger = DataFrame(Entity=fill(org), FiscalYear=fy, AccountingPeriod = ap, Actuals = actuals, Budget = budget)\n# fill(org) or org will produce same results\n\n# run one command at a time\n# adding one row at a time, can be done, but is not very efficient\npush!(df_Ledger, Dict(:Entity => \"Google\", :FiscalYear => 2022, \n        :AccountingPeriod => 1, :Actuals => 95.42, :Budget => 101))\npush!(df_Ledger, Dict(:Entity => \"Google\", :FiscalYear => 2022, \n        :AccountingPeriod => 2, :Actuals => 91.42, :Budget => 99))","category":"page"},{"location":"facts/#create-Finance-Data-model","page":"get Facts straight","title":"create Finance Data model","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Chart of accounts (organized hierarchy of account groups in tree form), Location/Department or Product based hierarchy allows businesses to group and report organization activities based on business processes.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"These hierarchical grouping help capture monetary and statistical values of organization in finance statements.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"To create Finance Data model and  Ledger Cash-flow or Balance Sheet like statements, We need associated dimensions (chartfields like chart of accounts).","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"We will discuss how to load actual data from CSV or RDBMS later. We will also learn how to group and create chartfield hierarchies later.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"But for now, first Let's start with creating fake ACCOUNT, department and location chartfields.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates\n# create dummy data\naccountsDF = DataFrame(\n    ENTITY = \"Apple Inc.\",\n    AS_OF_DATE=Date(\"1900-01-01\", dateformat\"y-m-d\"),\n    ID = 11000:1000:45000,\n    CLASSIFICATION=repeat([\n        \"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=5),\n    CATEGORY=[\n        \"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n        \"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n        \"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n        \"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n        \"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n        \"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n        \"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"],\n    STATUS=\"A\",\n    DESCR=repeat([\n    \"operating expenses\",\"non-operating expenses\",\"assets\",\"liability\",\"net-worth\",\"stats\",\"revenue\"], inner=5),\n    ACCOUNT_TYPE=repeat([\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"],inner=5));\n\nshow(\"Accounts DIM size is: \"), show(size(accountsDF)), show(\"Accounts Dim sample: \"), accountsDF[collect(1:5:35),:]","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"There is lot to unpack here in above Julia code and lot is wrong (not best practice for sure).","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"First, what is a dataframe anyway, think of Julia DataFrame as tabular representation of data arranged in rows and columns. Unlike SQL, you should get into habit of reading and writing one column at a time (not because of reason, you can't read/write rows) for faster performance. Each column is an Array or a list of values, referred as vector.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Above Julia code creates accounts dataframe with columns name as ASOFDATE, DESCR, CATEGORY, ACCOUNT_TYPE, CLASSIFICATION, STATUS.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"There are 35 rows, with same ASOFDATE, IDs starting from 11000-45000 in 1000 incremental values, all with STATUS = A (Active), 7 distinct Descriptions and account types (E=Expense, L=Liability, A= Assets, N=Net worth, S=Stats, R=Revenue) repeating 5 times per category.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"For 35 rows, it's fine to store data like this, but now is a good time to learn about Categorical and Pooled Arrays, in case when dataframe has millions of rows.","category":"page"},{"location":"facts/#Accounts-chartfield","page":"get Facts straight","title":"Accounts chartfield","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using Pkg, DataFrames, CategoricalArrays, PooledArrays, Dates\n\n# here CLASSIFICATION column vector stores 3500 distinct values in an array\nCLASSIFICATION=repeat([\"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=500)\n\ncl = categorical(CLASSIFICATION)\nlevels(cl)\n\n# using PooledArrays\npl = categorical(CLASSIFICATION)\nlevels(pl)\n\n# show values in tabular format\n# run one command at a time\ndf = DataFrame(Dict(\"Descr\" => \"CLASSIFICATION...ARR...\", \"Value\" => size(CLASSIFICATION)[1]))\npush!(df,(\"CAT...ARR...\",size(cl)[1]))\npush!(df,(\"CAT...ARR..COMPRESS.\",size(compress(cl))[1]))\npush!(df,(\"POOL...ARR...\",size(pl)[1]))\npush!(df,(\"POOL...ARR..COMPRESS.\",size(compress(pl))[1]))\npush!(df,(\"CAT...LEVELs...\",size(levels(cl))[1]))\npush!(df,(\"POOL...LEVELs...\",size(levels(pl))[1]))\npush!(df,(\"CLASSIFICATION...MEMSIZE\", Base.summarysize(CLASSIFICATION)))\npush!(df,(\"CAT...ARR...MEMSIZE\", Base.summarysize(cl)))\npush!(df,(\"POOL...ARR...MEMSIZE\", Base.summarysize(pl)))\npush!(df,(\"CAT...ARR..COMPRESS...MEMSIZE\", Base.summarysize(compress(cl))))\npush!(df,(\"POOL...ARR..COMPRESS...MEMSIZE\", Base.summarysize(compress(pl))))\n\nfirst(df,5)\n\naccountsDF = DataFrame(\n    ENTITY = \"Apple Inc.\",\n    AS_OF_DATE=Date(\"1900-01-01\", dateformat\"y-m-d\"),\n    ID = 11000:1000:45000,\n    CLASSIFICATION=repeat([\n        \"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=5),\n    CATEGORY=[\n        \"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n        \"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n        \"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n        \"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n        \"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n        \"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n        \"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"],\n    STATUS=\"A\",\n    DESCR=repeat([\n    \"operating expenses\",\"non-operating expenses\",\"assets\",\"liability\",\"net-worth\",\"stats\",\"revenue\"], inner=5),\n    ACCOUNT_TYPE=repeat([\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"],inner=5))","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Categorical and Pooled Arrays as name suggests, are data structure to store voluminous data efficiently,specially when a column in a data frame has small number of distinct values (aka levels), repeated across entire column vector.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"as an example, Finance Ledger may have millions of transactions and every row has one of these seven type of accounts. It's not recommended to store repeating value of entire string in every row. Instead, using a Categorical or PooledArray data type, memory/data size can be significantly reduced with out losing any data quality. (size(..) stays same for original, Categorical and PooledArray data type.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"as you can see in above example, size of categorical / pooled array data type matches with original column vector but significantly reduces size/memory of data. (Base.summarysize(...)) is reduced 50% and is further reduced by 85% if used with compress(...))","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Using Categorical Array type over PooledArray is recommended when there are fewer unique values, user need meaningful ordering and grouping. On the other hand, PoolArray is preferred when small memory usage is needed.","category":"page"},{"location":"facts/#Department-chartfield","page":"get Facts straight","title":"Department chartfield","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames\n## create Accounts chartfield\n# DEPARTMENT Chartfield\ndeptDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 1100:100:1500,\n    CLASSIFICATION=[\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"],\n    CATEGORY=[\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"],\n    STATUS=\"A\",\n    DESCR=[\n    \"Sales & Marketing\",\"Human Resource\",\"Infomration Technology\",\"Business leaders\",\"other temp\"\n        ],\n    DEPT_TYPE=[\"S\",\"H\",\"I\",\"B\",\"O\"]);\nsize(deptDF),deptDF[collect(1:5),:]","category":"page"},{"location":"facts/#Location-chartfield","page":"get Facts straight","title":"Location chartfield","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames\nlocationDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 11:1:22,\n    CLASSIFICATION=repeat([\n        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n    CATEGORY=repeat([\n        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n    STATUS=\"A\",\n    DESCR=[\n\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n\"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n\"Dallas\",\"San Francisco\"],\n    LOC_TYPE=\"Physical\");\nlocationDF[:,:]","category":"page"},{"location":"facts/#Ledger-FACT","page":"get Facts straight","title":"Ledger FACT","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"## pu\nusing DataFrames, Dates\naccountsDF = DataFrame(\n    ENTITY = \"Apple Inc.\",\n    AS_OF_DATE=Date(\"1900-01-01\", dateformat\"y-m-d\"),\n    ID = 11000:1000:45000,\n    CLASSIFICATION=repeat([\n        \"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=5),\n    CATEGORY=[\n        \"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n        \"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n        \"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n        \"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n        \"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n        \"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n        \"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"],\n    STATUS=\"A\",\n    DESCR=repeat([\n    \"operating expenses\",\"non-operating expenses\",\"assets\",\"liability\",\"net-worth\",\"stats\",\"revenue\"], inner=5),\n    ACCOUNT_TYPE=repeat([\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"],inner=5))\n# DEPARTMENT Chartfield\ndeptDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 1100:100:1500,\n    CLASSIFICATION=[\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"],\n    CATEGORY=[\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"],\n    STATUS=\"A\",\n    DESCR=[\n    \"Sales & Marketing\",\"Human Resource\",\"Infomration Technology\",\"Business leaders\",\"other temp\"\n        ],\n    DEPT_TYPE=[\"S\",\"H\",\"I\",\"B\",\"O\"]);\nsize(deptDF),deptDF[collect(1:5),:]\n\nlocationDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 11:1:22,\n    CLASSIFICATION=repeat([\n        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n    CATEGORY=repeat([\n        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n    STATUS=\"A\",\n    DESCR=[\n\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n\"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n\"Dallas\",\"San Francisco\"],\n    LOC_TYPE=\"Physical\");\nlocationDF[:,:]\n\n# creating Ledger\nledgerDF = DataFrame(\n            LEDGER = String[], FISCAL_YEAR = Int[], PERIOD = Int[], ORGID = String[],\n            OPER_UNIT = String[], ACCOUNT = Int[], DEPT = Int[], LOCATION = Int[],\n            POSTED_TOTAL = Float64[]\n            );\n\n# create 2020 Period 1-12 Actuals Ledger \nl = \"Actuals\";\nfy = 2020;\nfor p = 1:12\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# create 2021 Period 1-4 Actuals Ledger \nl = \"Actuals\";\nfy = 2021;\nfor p = 1:4\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# create 2021 Period 1-4 Budget Ledger \nl = \"Budget\";\nfy = 2021;\nfor p = 1:12\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# here is ~3 million rows ledger dataframe\nfirst(ledgerDF,5)","category":"page"},{"location":"facts/#Supply-chain-data-model","page":"get Facts straight","title":"Supply chain data model","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"you will need following packages.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using Pkg\nPkg.add(\"DataFrames\")\nPkg.add(\"Dates\")\nPkg.add(\"CategoricalArrays\")\nPkg.add(\"Interact\")\nPkg.add(\"WebIO\")\nPkg.add(\"CSV\")\nPkg.add(\"XLSX\")\nPkg.add(\"DelimitedFiles\")\nPkg.add(\"Distributions\")\nPkg.build(\"WebIO\")\nPkg.status();\n\nusing DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"rest of this blog, I will assume, you have added all packages and imported in current namespace/notebook scope.","category":"page"},{"location":"facts/#Supply-Chain-Data","page":"get Facts straight","title":"Supply Chain Data","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"We already covered DataFrames and ERP Finance data model in previous sections. in below section, let's recreate all Supply Chain DataFrames to continue advance analytics and visualization.","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"note: Note\nAll of Finance and supply chain data discussed here is also uploaded in GitHub repo under sampleData folder. This same script can be used to produce more voluminous data.","category":"page"},{"location":"facts/#Dimensions","page":"get Facts straight","title":"Dimensions","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"Item master, Item Attribs, Item Costing\nUNSPSC:  The United Nations Standard Products and Services Code® (UNSPSC®) is a global classification system of products and services.               These codes are used to classify products and services.\nGUDID: The Global Unique Device Identification Database (GUDID) is a database administered by the FDA that will serve as a reference catalog for every device with a unique device identifier (UDI).\nGTIN: Global Trade Item Number (GTIN) can be used by a company to uniquely identify all of its trade items. GS1 defines trade items as products or services that are priced, ordered or invoiced at any point in the supply chain.\nGMDN: The Global Medical Device Nomenclature (GMDN) is a comprehensive set of terms, within a structured category hierarchy, which name and group ALL medical device products including implantables, medical equipment, consumables, and diagnostic devices.\nVendor master, Vendor Attribs, Vendor Costing   Customer/Buyer/Procurement Officer Attribs   shipto, warehouse, storage & inventory locations","category":"page"},{"location":"facts/#Transactions","page":"get Facts straight","title":"Transactions","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"PurchaseOrder\nMSR - Material Service\nVoucher\nInvoice\nReceipt\nShipment\nSales, Revenue\nTravel, Expense, TimeCard\nAccounting Lines","category":"page"},{"location":"facts/#Item-Master","page":"get Facts straight","title":"Item Master","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"import Pkg\nPkg.add(\"XLSX\")\nPkg.add(\"CSV\")\nusing XLSX, CSV, DataFrames\n###############################\n## create SUPPLY CHAIN DATA ###\n###############################\n# Item master, Item Attribs, Item Costing ##\n#       UNSPSC, GUDID, GTIN, GMDN\n############################################\n\n##########\n# UNSPSC #\n##########\n# UNSPSC file can be downloaded from this link https://www.ungm.org/Public/UNSPSC\n# xf = XLSX.readxlsx(\"assets/sampleData/UNGM_UNSPSC_09-Apr-2022..xlsx\")\n# xf will display names of sheets and rows with data\n# let's read this data in to a DataFrame\n\n# using below command will read xlsx data into DataFrame but will not render column labels\n# df = DataFrame(XLSX.readdata(\"assets/sampleData/UNGM_UNSPSC_09-Apr-2022..xlsx\", \"UNSPSC\", \"A1:D12988\"), :auto)\n# dfUNSPSC = DataFrame(XLSX.readtable(\"assets/sampleData/UNGM_UNSPSC_09-Apr-2022..xlsx\", \"UNSPSC\")...)\n# ... operator will splat the tuple (data, column_labels) into the constructor of DataFrame\n\n# replace missing values with an integer 99999\n# replace!(dfUNSPSC.\"Parent key\", missing => 99999)\n\n# let's export this clean csv, we'll load this into database\n# CSV.write(\"UNSPSC.csv\", dfUNSPSC)\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfUNSPSC)\n# empty!(dfUNSPSC)\n# Base.summarysize(dfUNSPSC)\n\n# first(dfUNSPSC, 5)","category":"page"},{"location":"facts/#GUDID-Database","page":"get Facts straight","title":"GUDID Database","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n##########\n# GUDID ##\n##########\n# The complete list of GUDID Data Elements and descriptions can be found at this link.\n# https://www.fda.gov/media/120974/download\n# The complete GUDID Database (delimited version) download (250+MB)\n# https://accessgudid.nlm.nih.gov/release_files/download/AccessGUDID_Delimited_Full_Release_20220401.zip\n# let's extract all GUDID files in a folder\n# readdir(pwd())\n# readdir(\"assets/sampleData/GUDID\")\n# since these files are in txt (delimited) format, we'll use delimited pkg\n\n########################\n## large txt files #####\n## read one at a time ##\n########################\n\n# data, header = readdlm(\"assets/sampleData/GUDID/contacts.txt\", '|', header=true)\n# dfGUDIDcontacts = DataFrame(data, vec(header))\n\n# data, header = readdlm(\"assets/sampleData/GUDID/identifiers.txt\", '|', header=true)\n# dfGUDIDidentifiers = DataFrame(data, vec(header))\n\n# data, header = readdlm(\"assets/sampleData/GUDID/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfGUDIDcontacts),Base.summarysize(dfGUDIDidentifiers),Base.summarysize(dfGUDIDdevice)\n# empty!(dfGUDIDcontacts)\n# empty!(dfGUDIDidentifiers)\n# empty!(dfGUDIDdevice)\n# Base.summarysize(dfGUDIDcontacts),Base.summarysize(dfGUDIDidentifiers),Base.summarysize(dfGUDIDdevice)","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n##########\n# GTIN ###\n##########\n\n# xf = XLSX.readxlsx(\"assets/sampleData/DS_GTIN_ALL.xlsx\")\n# xf will display names of sheets and rows with data\n# let's read this data in to a DataFrame\n\n# using below command will read xlsx data into DataFrame but will not render column labels\n# df = DataFrame(XLSX.readdata(\"assets/sampleData/DS_GTIN_ALL.xlsx\", \"Worksheet\", \"A14:E143403   \"), :auto)\n# dfGTIN = DataFrame(XLSX.readtable(\"assets/sampleData/DS_GTIN_ALL.xlsx\", \"Worksheet\";first_row=14)...)\n# ... operator will splat the tuple (data, column_labels) into the constructor of DataFrame\n\n# replace missing values with an integer 99999\n# replace!(dfUNSPSC.\"Parent key\", missing => 99999)\n# size(dfUNSPSC)\n\n# let's export this clean csv, we'll load this into database\n# CSV.write(\"UNSPSC.csv\", dfUNSPSC)\n# readdir(pwd())\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfGTIN)\n# empty!(dfGTIN)\n# Base.summarysize(dfGTIN)","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n##########\n# GMDN ###\n##########\n\n## GMDN data is not available\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfGMDN)\n# empty!(dfGMDN)\n# Base.summarysize(dfGMDN)","category":"page"},{"location":"facts/#Vendor-Master","page":"get Facts straight","title":"Vendor Master","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n#################\n# Vendor master #\n#################\n# data, header = readdlm(\"assets/sampleData/GUDID/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# create Vendor Master from GUDID dataset\n# show(first(dfGUDIDdevice,5), allcols=true)\n# show(first(dfGUDIDdevice[:,[:brandName, :catalogNumber, :dunsNumber, :companyName, :rx, :otc]],5), allcols=true)\n# names(dfGUDIDdevice)\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :catalogNumber, :dunsNumber, :companyName, :rx, :otc]])\n# dfVendor = unique(dfGUDIDdevice[:,[:companyName]]) # 7574 unique vendors\n\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# dfVendor is a good dataset, have 216k rows for 7574 unique vendors\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfVendor)\n# empty!(dfVendor)\n# Base.summarysize(dfVendor)","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n#### Location Master\n\n#  data, header = readdlm(\"assets/sampleData/uscities.csv\", ',', header=true)\n#  dfLocation = DataFrame(data, vec(header))\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfLocation)\n# empty!(dfLocation)\n# Base.summarysize(dfLocation)","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"])","category":"page"},{"location":"facts/#creating-complete-Supply-Chain-Data-Model-DataFrames","page":"get Facts straight","title":"creating complete Supply Chain Data Model DataFrames","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"now since we created Supply chain attribute, chartfields / dimensions","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"item master\nvendor master\nlocation master\norg Hierarchy","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using above chartfields, let's create following Supply Chain Transactions","category":"page"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"MSR - Material Service request\nPurchaseOrder\nVoucher\nInvoice\nReceipt\nShipment\nSales, Revenue\nTravel, Expense, TimeCard\nAccounting Lines","category":"page"},{"location":"facts/#MSR-Material-Service-request","page":"get Facts straight","title":"MSR - Material Service request","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"assets/sampleData/GUDID/device.txt\", '|', header=true)\n# df GUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\n# dfOrgMaster = DataFrame(\n#    ENTITY=repeat([\"HeadOffice\"], inner=8),\n#    GROUP=repeat([\"Operations\"], inner=8),\n#    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n#    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"])\n\n# dfMSR = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    MSR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    FROM_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    TO_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(dfOrgMaster.UNIT, sampleSize));\n# first(dfMSR, 5)","category":"page"},{"location":"facts/#PO-Purchase-Order","page":"get Facts straight","title":"PO - Purchase Order","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"assets/sampleData/GUDID/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"]);\n\n# dfPO = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    PO_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    VENDOR=rand(unique(dfVendor.companyName), sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(1:150, sampleSize),\n#    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n#    );\n# show(first(dfPO, 5),allcols=true)","category":"page"},{"location":"facts/#Invoice-Voucher-Invoice","page":"get Facts straight","title":"Invoice - Voucher Invoice","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"assets/sampleData/GUDID/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"]);\n\n# dfVCHR = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    VCHR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    STATUS=rand([\"Closed\",\"Paid\",\"Open\",\"Cancelled\",\"Exception\"], sampleSize),\n#    VENDOR_INVOICE_NUM = rand(10001:9999999, sampleSize),\n#    VENDOR=rand(unique(dfVendor.companyName), sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(1:150, sampleSize),\n#    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n#    );\n# show(first(dfVCHR, 5),allcols=true)","category":"page"},{"location":"facts/#Sales-Revenue-Register","page":"get Facts straight","title":"Sales - Revenue Register","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"assets/sampleData/GUDID/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"]);\n\n# dfREVENUE = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    SALES_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    STATUS=rand([\"Sold\",\"Pending\",\"Hold\",\"Cancelled\",\"Exception\"], sampleSize),\n#    SALES_RECEIPT_NUM = rand(10001:9999999, sampleSize),\n#    CUSTOMER=rand(unique(dfVendor.companyName), sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(1:150, sampleSize),\n#    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n#    );\n# show(first(dfREVENUE, 5),allcols=true)","category":"page"},{"location":"facts/#Shipment-Receipt","page":"get Facts straight","title":"Shipment - Receipt","text":"","category":"section"},{"location":"facts/","page":"get Facts straight","title":"get Facts straight","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"assets/sampleData/GUDID/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"])\n\n# dfSHIPRECEIPT = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    SHIP_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    STATUS=rand([\"Shippped\",\"Returned\",\"In process\",\"Cancelled\",\"Exception\"], sampleSize),\n#    SHIPMENT_NUM = rand(10001:9999999, sampleSize),\n#    CUSTOMER=rand(unique(dfVendor.companyName), sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(1:150, sampleSize),\n#    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n#    );\n# show(first(dfSHIPRECEIPT, 5),allcols=true)","category":"page"},{"location":"#Billion-toilet-paper","page":"Introduction","title":"Billion $$ toilet paper","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"yes, you read this title right, Billion Dollar toilet paper.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This book is about Data Science, Graph Analysis to understand, how something like toilet paper is sold as gold during Pandemic. In next few chapters, we will use data science technologies to understand, predict and perhaps prevent global supply chain shortages specially for those items which every person needs to survive.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"#about-Author","page":"Introduction","title":"about Author","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nAuthor: Amit ShuklaBio: about meLast Update Date: Apr 04 2022Who should read this: small, medium, large ERP ConsultantsVersion: 0.22Sponsorship: open for funding","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: GitHub) (Image: YouTube) (Image: Twitter) (Image: LinkedIn) (Image: Medium)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"#how-to-use-this-book","page":"Introduction","title":"how to use this book","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This book first version is completely free(v1.2) and is published as website under GitHub gh-pages branch.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Most of the source code is MIT License, (except few ML/Deep Learning algorithms, which are proprietary and customer owned content).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Complete source code can be found here. https://github.com/AmitXShukla/P2P.ai","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"note: Note\nPlatform: TigerGraph, Oracle OCI, AWS, Google or Microsoft Azure data cloud.Analytics: Jupyter NBs, Julia Pluto notebooks, TigerGraph GSQL, Power BI, Tableau, Oracle Analytics Cloud or KibanaProgramming/Framework: Python, Julia, FluxML, TigerGraph GSQL, TigerGraph ML","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"#Procure2Pay.ai","page":"Introduction","title":"Procure2Pay.ai","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Procure to Pay Julia package provide a unified Analytics platform to support data analytical operations on all sort of Procurement, Accounts Payable, Procurement including Vendor, Use, Freight, Misc Tax Accruals data to address complete Buy to Pay data wrangling operations.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This package will provide a complete Analytic Software package, which can be deployed as a bolt-on or independent application for all data extract, load, transformation, ad-hoc reporting & Analytics, visualizations and tooling to support Data Science, AI, ML predictive Analytics.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This package is intended for small, medium, large and very Big Organizations who require a Big Data Tools which can ELT i.e. Extract very large amount of structured and unstructured data, load data into a uniform platform such as RDBMS, Hadoop Data Lake or non-SQL environment.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Further, advance data transformation wrangling techniques can be applied to prepare data for operations reporting, data analytic, advance data visualizations, data science operations including AI, ML for predictions.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This package also show case reporting, visualizations to support real time, live reporting on all mobile, web devices. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"#Table-of-Contents","page":"Introduction","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Pages = [\n\t\"index.md\"\n    \"define.md\"\n    \"path.md\"\n    \"process.md\"\n    \"facts.md\"\n    \"graph.md\"\n    \"query.md\"\n    \"analytics.md\"\n    \"ml.md\"\n    \"api.md\"\n]","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"note: ERP Systems\nP2P.jl package supports these ERP systems data structures.Oracle, PeopleSoft, SAP, Tally, Intuit, QuickBooks etc. I will cover examples from ERP Domains like GL (General Ledger), AP (Accounts Payable), AR (Account Receivables), B2P (Buy to Pay), Expense, Travel & Time, HCM Human Capital Management, CRM etc.","category":"page"},{"location":"process/#Business-process","page":"what do we know","title":"Business process","text":"","category":"section"},{"location":"process/","page":"what do we know","title":"what do we know","text":"let's us learn in detail about everything we already know.","category":"page"},{"location":"process/","page":"what do we know","title":"what do we know","text":"Most of the business's procurement operations are well defined, and for those organizations which don't, better need to prepare one first. A clear thoughtful defined process to manage Finances and Supply Chain operations are critical not only for organizations even for individuals and families in their day to day life.","category":"page"},{"location":"process/","page":"what do we know","title":"what do we know","text":"","category":"page"},{"location":"process/#operations-workflow","page":"what do we know","title":"operations workflow","text":"","category":"section"},{"location":"process/","page":"what do we know","title":"what do we know","text":"This diagram below shows at very high level, how most of the Procure to Pay operations are performed.","category":"page"},{"location":"process/","page":"what do we know","title":"what do we know","text":"In next chapter, we will read, write and understand these datasets in Julia language. ","category":"page"},{"location":"process/","page":"what do we know","title":"what do we know","text":"Once we understand these datasets, in following chapters, we will create Graphs, Vertices and Edges for data analysis, load actual data into Graphs. Which will lead us to perform end-to-end supply chain / procure to pay analytics for analysis, predictions and preventions insights.","category":"page"},{"location":"process/","page":"what do we know","title":"what do we know","text":"(Image: BusinessProcess)","category":"page"},{"location":"process/#Physical-ERD","page":"what do we know","title":"Physical ERD","text":"","category":"section"},{"location":"process/","page":"what do we know","title":"what do we know","text":"(Image: ERD)","category":"page"},{"location":"process/","page":"what do we know","title":"what do we know","text":"(Image: ERD)","category":"page"}]
}
